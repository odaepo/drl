{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"NKcRV4mH5_Kv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648219450328,"user_tz":-60,"elapsed":16978,"user":{"displayName":"Carlo Metta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhML7AVq4zpggMmz58dO0-jrr7w6yGgUhfWjryXqA=s64","userId":"15373306937251205392"}},"outputId":"bb77d720-208b-4393-8fda-632b19f924ad"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"5nPXX-i5gGXH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648219548195,"user_tz":-60,"elapsed":244,"user":{"displayName":"Carlo Metta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhML7AVq4zpggMmz58dO0-jrr7w6yGgUhfWjryXqA=s64","userId":"15373306937251205392"}},"outputId":"b861cb47-1c3d-4223-fe70-5b6303508cb2"},"source":["!ls -lah '/content/gdrive/My Drive/Colab Notebooks/didattica/gym/dennybritz-modified/reinforcement-learning-master/lib/envs'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 19K\n","-rw------- 1 root root 4.2K Mar 25 09:23 blackjack.py\n","-rw------- 1 root root 2.7K Mar 25 09:23 cliff_walking.py\n","-rw------- 1 root root 3.8K Mar 25 09:23 gridworld.py\n","-rw------- 1 root root    0 Mar 25 09:23 __init__.py\n","drwx------ 2 root root 4.0K Mar 25 09:24 __pycache__\n","-rw------- 1 root root 2.6K Mar 25 09:23 windy_gridworld.py\n"]}]},{"cell_type":"code","metadata":{"id":"1dq4oFWBWsAG"},"source":["import numpy as np\n","import sys\n","if \"/content/gdrive/My Drive/Colab Notebooks/didattica/gym/dennybritz-modified/reinforcement-learning-master/\" not in sys.path:\n","  sys.path.append(\"/content/gdrive/My Drive/Colab Notebooks/didattica/gym/dennybritz-modified/reinforcement-learning-master/\")\n","from lib.envs.gridworld import GridworldEnv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"POXD47GDlEkF"},"source":["env = GridworldEnv()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CABei0dalEkM"},"source":["def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n","    \"\"\"\n","    Evaluate a policy given an environment and a full description of the environment's dynamics.\n","\n","    Args:\n","        policy: [S, A] shaped matrix representing the policy.\n","        env: OpenAI env. env.P represents the transition probabilities of the environment.\n","            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n","            env.nS is a number of states in the environment.\n","            env.nA is a number of actions in the environment.\n","        theta: We stop evaluation once our value function change is less than theta for all states.\n","        discount_factor: Gamma discount factor.\n","\n","    Returns:\n","        Vector of length env.nS representing the value function.\n","    \"\"\"\n","    # Start with a random (all 0) value function\n","    V = np.zeros(env.nS)\n","    # Implement!\n","    return np.array(V)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EsOp1t7olEkT"},"source":["random_policy = np.ones([env.nS, env.nA]) / env.nA\n","v = policy_eval(random_policy, env)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["v"],"metadata":{"id":"AxRVMAne0ZFx","executionInfo":{"status":"ok","timestamp":1648224213896,"user_tz":-60,"elapsed":219,"user":{"displayName":"Carlo Metta","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhML7AVq4zpggMmz58dO0-jrr7w6yGgUhfWjryXqA=s64","userId":"15373306937251205392"}},"outputId":"815d7326-ce78-41a0-99a0-309ed5827c47","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  0.        , -13.99993529, -19.99990698, -21.99989761,\n","       -13.99993529, -17.9999206 , -19.99991379, -19.99991477,\n","       -19.99990698, -19.99991379, -17.99992725, -13.99994569,\n","       -21.99989761, -19.99991477, -13.99994569,   0.        ])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"WMXSVJ75lEkb"},"source":["# Test: Make sure the evaluated policy is what we expected\n","expected_v = np.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n","np.testing.assert_array_almost_equal(v, expected_v, decimal=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ds0VFjPclEkj"},"source":[],"execution_count":null,"outputs":[]}]}